{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import igraph\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh, eigs\n",
    "from scipy.sparse import csgraph\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pymongo\n",
    "\n",
    "try:\n",
    "    client = MongoClient('localhost', 27017) # host, port\n",
    "except ConnectionFailure as e:\n",
    "    sys.stderr.write(\"Could not connect to MongoDB: %s\" % e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_tweets',\n",
       " 'filtered_tweets_noRetweets_english',\n",
       " 'english_tweets',\n",
       " 'english_noRetweet_tweets',\n",
       " 'users',\n",
       " 'manually_labelled_tweets',\n",
       " 'filtered_noRetweets_english_onlyPersonal']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get database with all tweets\n",
    "#db = client.tweets_database\n",
    "\n",
    "\n",
    "#db.filtered_tweets.drop()\n",
    "\n",
    "# new database with cleaned tweets (without Retweets, in english)\n",
    "english_tweets = client.tweets_database.english_tweets\n",
    "\n",
    "client.tweets_database.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.tweet_database.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'userName_retweetName_nWeek'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#pymongo.version\n",
    "#client.tweets_database.english_tweets.create_index( [{ 'user.screen_name': 1 , \n",
    "#                                                               'retweeted_status.user.screen_name': 1,\n",
    "#                                                               'number_of_weeks': 1}] )\n",
    "client.tweets_database.english_tweets.create_index([('user.screen_name', 1), \n",
    "                                                             ('retweeted_status.user.screen_name', 1),\n",
    "                                                             ('number_of_weeks', 1)], \n",
    "                                                              name='userName_retweetName_nWeek')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2922\n"
     ]
    }
   ],
   "source": [
    "users = set().union( [v['_id'] for v in english_tweets.aggregate( [ \n",
    "                                            {'$match': {'number_of_weeks' : 1}},\n",
    "                                            {\"$group\" : { \"_id\" : \"$user.screen_name\" } }\n",
    "                                           \n",
    "                                         ] )\n",
    "                      ],\n",
    "                     [v['_id'] for v in english_tweets.aggregate( [ \n",
    "                                            {'$match': {'number_of_weeks' : 1}},\n",
    "                                            {\"$group\" : { \"_id\" : \"$retweeted_status.user.screen_name\" } }\n",
    "                                           \n",
    "                                         ] )\n",
    "                      ]\n",
    "                   )\n",
    "print(len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dictionary: each user name corresponds to one unique number between 0 and n_users\n",
    "\n",
    "count = 0\n",
    "userToInt_dict = {}\n",
    "for user in users:\n",
    "    userToInt_dict[user] = count\n",
    "    count += 1\n",
    "#pprint(userToInt_dict)\n",
    "\n",
    "intToUser_dict = {v: k for k,v in userToInt_dict.items()}\n",
    "#intToUser_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(M_sp, intToUser_dict):\n",
    "    \n",
    "    # get total number of nodes/users\n",
    "    N = M_sp.get_shape()[0]\n",
    "    \n",
    "    # calculate connected components of sparse matrix\n",
    "    connected_components = csgraph.connected_components(M_sp, directed=True)\n",
    "    print(\"connected_components:\", connected_components[0])\n",
    "    \n",
    "    # count number of nodes/users in each component\n",
    "    countFrequenciesComponents = np.bincount(connected_components[1])\n",
    "    #print(\"countFrequenciesComponents:\", countFrequenciesComponents)\n",
    "\n",
    "    unique, counts = np.unique(countFrequenciesComponents, return_counts=True)\n",
    "    #print(\"Count occurences of groups\", dict(zip(unique, counts)))\n",
    "    \n",
    "    # sort connected components from most connected to not connected at all\n",
    "    sorted_labels = np.argsort(countFrequenciesComponents)[::-1]\n",
    "    #print(\"sorted labels:\", sorted_labels)\n",
    "    \n",
    "    # define vector containing all scores for each node\n",
    "    v = pd.DataFrame(columns=[\"score\"])\n",
    "\n",
    "    for ind in sorted_labels:\n",
    "        # when we see a single component of  size one we can break, \n",
    "        #since all other components after will be one too (array is sorted!)\n",
    "        if countFrequenciesComponents[ind] != 1:\n",
    "            #print(\"ind:\", ind)\n",
    "            #print(\"countfreq:\", countFrequenciesComponents[ind])\n",
    "            N_comp = countFrequenciesComponents[ind]\n",
    "\n",
    "            # get list of indices of the current connected composent representing the users\n",
    "            indices_user_comp = [i for i, comp in enumerate(connected_components[1]) if comp == ind]\n",
    "            #print(\"length of indices_ user:\", len(indices_user_comp))\n",
    "\n",
    "\n",
    "            # get submatrix of current component\n",
    "            #submatrix = M_sp.toarray()[connected_components[1] == ind, :][:, connected_components[1] == ind]\n",
    "            submatrix = M_sp.tocsr()[connected_components[1] == ind, :][:, connected_components[1] == ind]\n",
    "        \n",
    "            #print(\"shape submatrix:\", submatrix.shape)\n",
    "            #print(submatrix)\n",
    "\n",
    "            # k must be less than ndim(A)-1\n",
    "            #eigenvalues, eigenvectors = np.linalg.eig(submatrix)\n",
    "            if submatrix.get_shape()[0] > 10:\n",
    "                eigenvalues, eigenvectors = eigs(submatrix, k=2, which='LM')\n",
    "            else:\n",
    "                eigenvalues, eigenvectors = np.linalg.eig(submatrix.toarray())\n",
    "\n",
    "\n",
    "            # get index of maximum eigen value\n",
    "            max_eigval_ind = np.argmax(eigenvalues)\n",
    "            #print(\"max eigen value:\", eigenvalues[max_eigval_ind], \", index:\", max_eigval_ind)\n",
    "            #print(\"max eigen vector:\")\n",
    "            #print(eigenvectors[:,max_eigval_ind])\n",
    "\n",
    "            # normalise eigen vector for maximum eigen value s.t. sum of vector = 1\n",
    "            max_eigvec_norm = eigenvectors[:,max_eigval_ind]/sum(eigenvectors[:,max_eigval_ind])\n",
    "            #print(\"normalised max eigen vector:\")\n",
    "            #print(max_eigvec_norm)\n",
    "\n",
    "            # weight-average vectors s.t. score of each node is weighted according to the size of the component\n",
    "            # which it is found, thus ensuring that the scores are comparable and that we do not discard\n",
    "            # information from the smaller components\n",
    "            users_of_component = [intToUser_dict[user] for user in indices_user_comp]\n",
    "\n",
    "            max_eigvec_norm_weighted = pd.DataFrame((N_comp/N)*max_eigvec_norm, index=users_of_component,\n",
    "                                                    columns=[\"score\"])\n",
    "            #print(\"Weighted eig vec:\")\n",
    "            #print(max_eigvec_norm_weighted)\n",
    "            #print(\"\")\n",
    "\n",
    "            # append scores from current connected composant to global vector v containing all scores\n",
    "            v = pd.concat([v,max_eigvec_norm_weighted], axis=0)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dictionary in which we save the scores over the weeks\n",
    "authority_dict = {}\n",
    "hub_dict = {}\n",
    "\n",
    "\n",
    "nWeek = 1\n",
    "while(True):\n",
    "    print(\"----------------WEEK {} -----------------:\".format(nWeek))\n",
    "    # get unique users and users of the retweets twittering in week 1\n",
    "    #users_tweet = english_tweets.distinct('user.screen_name', {'number_of_weeks' : nWeek}) #user.id\n",
    "    #users_retweet = english_tweets.distinct('retweeted_status.user.screen_name', \n",
    "    #                                                 {'number_of_weeks' : nWeek})\n",
    "\n",
    "    #users = set().union(users_tweet, users_retweet)\n",
    "    \n",
    "    users = set().union( [v['_id'] for v in english_tweets.aggregate( [ \n",
    "                                                {'$match': {'number_of_weeks' : nWeek}},\n",
    "                                                {\"$group\" : { \"_id\" : \"$user.screen_name\" } }\n",
    "\n",
    "                                             ] )\n",
    "                          ],\n",
    "                         [v['_id'] for v in english_tweets.aggregate( [ \n",
    "                                                {'$match': {'number_of_weeks' : nWeek}},\n",
    "                                                {\"$group\" : { \"_id\" : \"$retweeted_status.user.screen_name\" } }\n",
    "\n",
    "                                             ] )\n",
    "                          ]\n",
    "                       )\n",
    "\n",
    "    n_users = len(users)\n",
    "    #print(\"number of users:\", n_users)\n",
    "\n",
    "    print(\"INFO: Number of nodes total:\", n_users)\n",
    "\n",
    "    \n",
    "    # create dictionary: each user name corresponds to one unique number between 0 and n_users\n",
    "\n",
    "    count = 0\n",
    "    userToInt_dict = {}\n",
    "    for user in users:\n",
    "        userToInt_dict[user] = count\n",
    "        count += 1\n",
    "\n",
    "    intToUser_dict = {v: k for k,v in userToInt_dict.items()}\n",
    "    #intToUser_dict\n",
    "    print(\"INFO: Created dictionaries for conversion integer <-> user\")\n",
    "    print(\"\\t Length of dict:\", len(intToUser_dict))\n",
    "    \n",
    "    # break at last week when there is no tweet anymore\n",
    "    if n_users < 20:\n",
    "        break\n",
    "      \n",
    "    \n",
    "    # create matrix users x users \n",
    "    #A_sp = sp.lil_matrix(np.zeros((n_users, n_users)))#.toarray()\n",
    "    A_sp = sp.lil_matrix(sp.coo_matrix((n_users, n_users)))#.toarray()\n",
    "\n",
    "    # fill retweet adjacency matrix with number of retweets\n",
    "    tweets_per_week = english_tweets.find({'number_of_weeks' : nWeek}, {\"user.screen_name\":1, \"retweeted_status.user.screen_name\":1})\n",
    "    #print(\"INFO: got tweets per week, size:\", sys.getsizeof(tweets_per_week))\n",
    "\n",
    "    i = 1\n",
    "    for tweet in tweets_per_week:\n",
    "        # only for retweets: add +1 when user of tweet retweeted other user\n",
    "        if \"retweeted_status\" in tweet.keys():\n",
    "            who_retweeted = userToInt_dict[tweet[\"user\"][\"screen_name\"]]\n",
    "            retweeted_from = userToInt_dict[tweet[\"retweeted_status\"][\"user\"][\"screen_name\"]]\n",
    "            A_sp[who_retweeted, retweeted_from] += 1\n",
    "\n",
    "       \n",
    "    print(\"INFO: number of non zero entries in adjacency matr:\", A_sp.count_nonzero())        \n",
    "    print(\"INFO: Memory usage of A (in KB):\", A_sp.data.nbytes/1024)\n",
    "\n",
    "    \n",
    "    N = n_users\n",
    "    \n",
    "\n",
    "    # Co-citation network\n",
    "    # ----------------------------------------------------------------\n",
    "    C = A_sp.transpose() * A_sp\n",
    "\n",
    "    score_authority = calculate_score(C, intToUser_dict)\n",
    "    #print(\"INFO: Score calculated:\")\n",
    "    #print(score_authority)\n",
    "    print(\"Authority score length: \", len(score_authority))\n",
    "    \n",
    "    \n",
    "    for user, row in score_authority.iterrows():\n",
    "        if user not in authority_dict.keys():\n",
    "            authority_dict[user] = [(nWeek, row.score)]\n",
    "        else:\n",
    "            authority_dict[user].append((nWeek, row.score))\n",
    "    \n",
    "    \n",
    "    # Bibliographic network\n",
    "    # ------------------------------------------------------------------\n",
    "#    B = A_sp * A_sp.transpose()\n",
    "    \n",
    "#    score_hub = calculate_score(B, intToUser_dict)\n",
    "#    print(\"INFO: Score calculated\")\n",
    "    #print(score_authority)\n",
    "#    print(\"\\tlength: \", len(score_hub))\n",
    "\n",
    "#    for user, row in score_hub.iterrows():\n",
    "#        if user not in hub_dict.keys():\n",
    "#            hub_dict[user] = [(nWeek, row.score)]\n",
    "#        else:\n",
    "#            hub_dict[user].append((nWeek, row.score))\n",
    "    \n",
    "    nWeek += 1\n",
    "    print(\"\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authority_dict\n",
    "\n",
    "#print(datetime.datetime.now().strftime(format))\n",
    "for k, v in authority_dict.items():\n",
    "    s = k+\";\"\n",
    "    for score in v:\n",
    "        s += str(score[0])+\";\"+str(score[1])+\";\"\n",
    "\n",
    "    s += \"\\n\"\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "format = \"%Y-%m-%d_%H-%M-%S\"\n",
    "\n",
    "pathname = \"authority_scores_week13_33__{}.csv\".format(datetime.datetime.now().strftime(format))\n",
    "with open(pathname,'w') as file:\n",
    "    for k, v in authority_dict.items():\n",
    "        s = k+\";\"\n",
    "        for score in v:\n",
    "            s += str(score[0])+\";\"+str(score[1])+\";\"\n",
    "\n",
    "        s += \"\\n\"\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authority_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "authority_dict_true = authority_dict.copy()\n",
    "with open(\"authority_scores_week1_12__2018-05-23_15-52-01.csv\", 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.split(\";\")\n",
    "        key = line[0]\n",
    "        value = line[1:-1]\n",
    "        \n",
    "        i = 0\n",
    "        temp = []\n",
    "        while i < len(value):\n",
    "            temp.append((value[i], value[i+1]))\n",
    "            i += 2 \n",
    "        value = temp\n",
    "        \n",
    "        if key in authority_dict.keys():\n",
    "            for tup in authority_dict[key]:\n",
    "                value.append(tup)\n",
    "            authority_dict[key] = value\n",
    "        else:\n",
    "            authority_dict[key] = value\n",
    "            \n",
    "        #print(key)\n",
    "        #print(value)\n",
    "        \n",
    "\n",
    "  \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authority_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = \"%Y-%m-%d_%H-%M-%S\"\n",
    "\n",
    "pathname = \"authority_scores__{}.csv\".format(datetime.datetime.now().strftime(format))\n",
    "with open(pathname,'w') as file:\n",
    "    for k, v in authority_dict.items():\n",
    "        s = k+\";\"\n",
    "        for score in v:\n",
    "            s += str(score[0])+\";\"+str(score[1])+\";\"\n",
    "\n",
    "        s += \"\\n\"\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate aggregated score\n",
    "aggregated_scores = []\n",
    "for key, values in authority_dict.items():\n",
    "    print(key)\n",
    "    print(values)\n",
    "    sum_scores = 0\n",
    "    for tup in values:\n",
    "        if isinstance(tup[1], str) :\n",
    "            sum_scores += np.complex(tup[1]).real\n",
    "        else:\n",
    "            sum_scores += np.array(tup[1]).real\n",
    "        \n",
    "    print(sum_scores)\n",
    "    print(len(values))\n",
    "    print(\"\")\n",
    "\n",
    "    aggregated_scores.append((key, sum_scores, len(values)))\n",
    "    #break\n",
    "aggregated_scores = sorted(aggregated_scores, key=lambda x: x[1], reverse=True)\n",
    "aggregated_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider only scores of people with at least tweets of 10 weeks\n",
    "#aggregated_scores_filtered = [tup for tup in aggregated_scores if tup[2] > 15]\n",
    "aggregated_scores_filtered = [tup for tup in aggregated_scores]\n",
    "\n",
    "#aggregated_scores_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_scores_filtered[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "format = \"%Y-%m-%d_%H-%M-%S\"\n",
    "\n",
    "pathname = \"authority_scores_sorted_all_{}.csv\".format(datetime.datetime.now().strftime(format))\n",
    "with open(pathname,'w') as file:\n",
    "    file.write(\"User;score;number_of_weeks\\n\")\n",
    "    for tup1, tup2, tup3 in aggregated_scores_filtered:\n",
    "        s = str(tup1)+\";\"+str(tup2)+\";\"+str(tup3)+\";\"+\"\\n\"\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
