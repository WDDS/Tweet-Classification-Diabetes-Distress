{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\stagiaire2014\\miniconda3\\envs\\deepscience\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "c:\\users\\stagiaire2014\\miniconda3\\envs\\deepscience\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import igraph\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pymongo\n",
    "import gensim\n",
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from html import unescape\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    " \n",
    "\n",
    "try:\n",
    "    client = MongoClient('localhost', 27017) # host, port\n",
    "except ConnectionFailure as e:\n",
    "    sys.stderr.write(\"Could not connect to MongoDB: %s\" % e)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_tweets',\n",
       " 'english_tweets',\n",
       " 'english_noRetweet_tweets',\n",
       " 'users',\n",
       " 'manually_labelled_tweets',\n",
       " 'filtered_noRetweets_english_onlyPersonal']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get database with all tweets\n",
    "db = client.tweets_database\n",
    "\n",
    "#client.tweets_database.manually_labelled_tweets.drop()\n",
    "\n",
    "\n",
    "#filtered_noRetweets_english_onlyPersonal = client.tweets_database.filtered_noRetweets_english_onlyPersonal\n",
    "\n",
    "english_noRetweet_tweets = db.english_noRetweet_tweets\n",
    "\n",
    "manually_labelled_tweets = db.manually_labelled_tweets\n",
    "\n",
    "\n",
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Stagiaire2014\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Stagiaire2014\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Stagiaire2014\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "path = 'D:\\A_AHNE1\\Tweet-Classification-Diabetes-Distress\\preprocess'\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.insert(0, path)\n",
    "sys.path\n",
    "\n",
    "from preprocess import Preprocess\n",
    "prep = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>personal (0=no, 1=yes)</th>\n",
       "      <th>tweet_proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>_JenniferLyne</td>\n",
       "      <td>Seven yoga poses for diabetes https://t.co/B68...</td>\n",
       "      <td>0</td>\n",
       "      <td>['seven', 'yoga', 'pose', 'diabet', 'URL']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>marguerite3636d</td>\n",
       "      <td>#reversediabetes now ➤ https://t.co/PDEsqoaou8...</td>\n",
       "      <td>0</td>\n",
       "      <td>['reversediabet', 'URL', 'could', 'tb', 'vacci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>amandajohnjones</td>\n",
       "      <td>The occurrence of #hypoglycemia can pose a ser...</td>\n",
       "      <td>0</td>\n",
       "      <td>['occurr', 'hypoglycemia', 'pose', 'serious', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Healthcheqs</td>\n",
       "      <td>Others talk - we help! Improve public #health ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['other', 'talk', 'help', 'improv', 'public', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>70_130</td>\n",
       "      <td>A three-month road trip (by a real diabetic) t...</td>\n",
       "      <td>1</td>\n",
       "      <td>['three-month', 'road', 'trip', 'real', 'diabe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        user_name  \\\n",
       "0           0    _JenniferLyne   \n",
       "1           1  marguerite3636d   \n",
       "2           2  amandajohnjones   \n",
       "3           3      Healthcheqs   \n",
       "4           4           70_130   \n",
       "\n",
       "                                               tweet  personal (0=no, 1=yes)  \\\n",
       "0  Seven yoga poses for diabetes https://t.co/B68...                       0   \n",
       "1  #reversediabetes now ➤ https://t.co/PDEsqoaou8...                       0   \n",
       "2  The occurrence of #hypoglycemia can pose a ser...                       0   \n",
       "3  Others talk - we help! Improve public #health ...                       0   \n",
       "4  A three-month road trip (by a real diabetic) t...                       1   \n",
       "\n",
       "                                          tweet_proc  \n",
       "0         ['seven', 'yoga', 'pose', 'diabet', 'URL']  \n",
       "1  ['reversediabet', 'URL', 'could', 'tb', 'vacci...  \n",
       "2  ['occurr', 'hypoglycemia', 'pose', 'serious', ...  \n",
       "3  ['other', 'talk', 'help', 'improv', 'public', ...  \n",
       "4  ['three-month', 'road', 'trip', 'real', 'diabe...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_csv = pd.read_csv(\"D:\\A_AHNE1\\Tweet-Classification-Diabetes-Distress\\manually_labeled_tweets_users_instVSpers_noNeg.csv\",\n",
    "                         sep=\";\")\n",
    "tweets_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tweets_csv[\"personal (0=no, 1=yes)\"]\n",
    "tweets = tweets_csv[\"tweet_proc\"]\n",
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVectorizer(CountVectorizer):\n",
    "    # overwrite the build_analyzer method, allowing one to\n",
    "    # create a custom analyzer for the vectorizer\n",
    "        def build_analyzer(self):\n",
    "            # load stop words using CountVectorizer's built in method\n",
    "            stop_words = self.get_stop_words()\n",
    "            \n",
    "            # create the analyzer that will be returned by this method\n",
    "            def analyser(tweet):\n",
    "                #print(\"tweet\")\n",
    "                #print(tweet)\n",
    "\n",
    "                tweet = prep.replace_contractions(tweet)\n",
    "                tweet = prep.replace_hashtags_URL_USER(tweet)\n",
    "                tweet = prep.tokenize(tweet)\n",
    "                tweet = prep.remove_punctuation(tweet)\n",
    "\n",
    "                tweet = prep.preprocess_emojis(tweet)\n",
    "                tweet = prep.preprocess_emoticons(tweet)\n",
    "                tweet = prep.remove_non_ascii(tweet)\n",
    "                tweet = prep.to_lowercase(tweet)\n",
    "\n",
    "                tweet = prep.remove_stopwords(tweet)\n",
    "                tweet = prep.lemmatize_verbs(tweet)\n",
    "                tweet = prep.stem_words(tweet)\n",
    "#                tweet = [ x for x in tweet if x not in[\"diabet\", \"glucos\", \"insulin\", \"type\", \"1\", \"2\", \"\", \"get\", \"sugar\", \"would\",\n",
    "#                                                      \"go\", \"know\", \"take\", \"give\", \"say\", \"one\", \"could\", \"would\", \"people\", \"look\",\n",
    "#                                                      \"year\", \"test\", \"see\", \"oh\", \"via\", \"bitch\", \"daddi\", \"hi\", \"w\", \"b\", \"n\", \"c\",\n",
    "#                                                      \"ii\", \"dr\", \"rt\", \"bc\", \"ok\", \"think\", \"make\"] ]\n",
    "\n",
    "                return tweet\n",
    "            \n",
    "            return(analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df : ignores terms that have a document frequency strictly lower than the given threshold (absolute counts)\n",
    "# max_df : ignores terms that have a document frequency strictly higher than the given threshold (proportion of documents)\n",
    "vectorizer_test = CustomVectorizer(min_df=5, max_df=0.9, \n",
    "                                     stop_words=None, lowercase=True, analyzer = 'word' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MultinomialNB()\n",
    "#model = SVC()\n",
    "model = RandomForestClassifier()\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('model', model),\n",
    "])\n",
    "\n",
    "\n",
    "#text_clf = text_clf.fit(tweets, labels)\n",
    "#predicted = text_clf.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84090909 0.82575758 0.78787879 0.78030303 0.81818182 0.88549618\n",
      " 0.83969466 0.85496183 0.90076336 0.70769231]\n",
      "0.8241638641256961\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(text_clf, tweets, labels, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vect__min_df' : [1, 5],\n",
    "              'vect__max_df' : [0.9, 1.0],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              \n",
    "              # param for MultinomialNB\n",
    "              #'model__alpha': (10, 1, 1e-1, 1e-2, 1e-3),\n",
    "              \n",
    "              # param for SVC\n",
    "              #'model__kernel' : [\"linear\", \"poly\", \"rbf\"],\n",
    "              #'model__C' : [0.1, 1.0, 5, 10, 15],\n",
    "              #'model__tol' : [1e-5, 1e-4, 1e-3],\n",
    "              \n",
    "              # param for RandomForestClassifier\n",
    "              'model__n_estimators' : [20, 30, 40],\n",
    "              'model__criterion' : ['gini', 'entropy'],\n",
    "              'model__max_features' : ['auto', 'log2', None],\n",
    "              'model__max_depth' : [10, 20, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 864 candidates, totalling 8640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   24.1s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:   28.8s\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1426 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1953 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2560 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3249 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4018 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4869 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5800 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6813 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7906 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8640 out of 8640 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best: 0.838661 using {'model__n_estimators': 40, 'model__max_features': 'log2', 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'model__criterion': 'entropy', 'tfidf__use_idf': False, 'model__max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(text_clf, parameters, cv=10, n_jobs=-1, verbose=2)\n",
    "grid = grid.fit(tweets, labels)\n",
    "\n",
    "#print(grid.cv_results_)\n",
    "print(\"\\nBest: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "\n",
    "# with dataset where single tweets are labeled\n",
    "# MultinomialNB: Best: 0.880652 using {'tfidf__use_idf': False, 'model__alpha': 1, 'vect__ngram_range': (1, 1)}\n",
    "# SVC : Best: 0.893996 using {'vect__ngram_range': (1, 2), 'model__C': 5, 'model__kernel': 'linear', 'model__tol': 1e-05, 'tfidf__use_idf': True}\n",
    "# RandomForest: Best: 0.868792 using {'model__n_estimators': 20, 'model__max_features': 'auto', 'vect__ngram_range': (1, 1), 'model__criterion': 'entropy', 'tfidf__use_idf': False, 'model__max_depth': 30}\n",
    "\n",
    "# with dataset where users are labeled\n",
    "# MultinomialNB: Best: 0.840183 using {'tfidf__use_idf': False, 'model__alpha': 1, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__max_df': 0.9}\n",
    "# SVC : Best: 0.841705 using {'model__kernel': 'linear', 'vect__min_df': 1, 'vect__ngram_range': (1, 2), 'vect__max_df': 0.9, 'model__C': 1.0, 'tfidf__use_idf': True, 'model__tol': 1e-05}\n",
    "# RandomForest: Best: 0.838661 using {'model__n_estimators': 40, 'model__max_features': 'log2', 'vect__min_df': 5, 'vect__ngram_range': (1, 1), 'vect__max_df': 1.0, 'model__criterion': 'entropy', 'tfidf__use_idf': False, 'model__max_depth': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[574,  95],\n",
       "       [128, 517]], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = MultinomialNB()\n",
    "model = SVC()\n",
    "#model = RandomForestClassifier()\n",
    "text_clf = Pipeline([('vect', CountVectorizer(min_df=1, max_df=0.9)),\n",
    "                      ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                      #('model', MultinomialNB(alpha=1)),\n",
    "                      ('model', SVC(C=1, kernel='linear', tol=1e-5)),\n",
    "                      #('model', LinearSCV()),\n",
    "])\n",
    "\n",
    "\n",
    "y_pred = cross_val_predict(text_clf, tweets, labels, cv=10)\n",
    "conf_mat = confusion_matrix(labels, y_pred)\n",
    "conf_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[574  95]\n",
      " [128 517]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "plt.figure()\n",
    "plot_confusion_matrix(conf_mat, classes=[\"institutional\", \"personal\"], title=\"confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.81766382, 0.84477124]),\n",
       " array([0.85799701, 0.80155039]),\n",
       " array([0.837345  , 0.82259348]),\n",
       " array([669, 645], dtype=int64))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get FP and FN to check the wrong classified tweets\n",
    "#conf_mat = confusion_matrix(labels, y_pred)\n",
    "\n",
    "y_pred_pd = pd.DataFrame(y_pred, columns=[\"y_pred\"])\n",
    "tweets_compare = pd.concat([tweets_csv, y_pred_pd], axis=1)\n",
    "\n",
    "tweets_compare = tweets_compare[tweets_compare[\"personal (0=no, 1=yes)\"] != tweets_compare[\"y_pred\"]]\n",
    "\n",
    "tweets_compare.head(20)\n",
    "tweets_compare.to_csv(\"compare_tweets.csv\", sep=\";\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_personal = tweets_csv[tweets_csv[\"personal (0=no, 1=yes)\"]==1]\n",
    "tweets_institut = tweets_csv[tweets_csv[\"personal (0=no, 1=yes)\"]==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three-month', 'road', 'trip', 'real', 'diabet', 'rais', 'money', 'jdrf', 'donat', 'URL']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-6eff927f313f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;34m\"USER\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mc_ME_USER_URLnotextern\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;32melif\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"entities\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"urls\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"expanded_url\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://twitter.com\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mc_ME_USER_URLnotextern\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "x = ast.literal_eval(tweets_institut[\"tweet_proc\"][0])\n",
    "[n.strip() for n in x]\n",
    "\n",
    "personal = [\"me\", \"I\", \"i\", \"I'm\", \"i'm\", \"Im\", \"im \", \"my\", \"My\", \"I've\", \"i've\", \"I'd\", \"i'd\"] \n",
    "\n",
    "def check_personal_in_tweet(tweet):\n",
    "    #print(tweet)\n",
    "    for p in personal:\n",
    "    #    print(\"p:\", p)\n",
    "        if p in tweet:\n",
    "    #        print(\"yeah\")\n",
    "            return True\n",
    "    #print(\"no\")\n",
    "    return False \n",
    "        \n",
    "c_USER = 0\n",
    "c_USER_START = 0\n",
    "c_URL = 0\n",
    "c_URL_END = 0\n",
    "c_ME = 0\n",
    "c_USER_ME = 0\n",
    "c_ME_USER_URLnotextern = 0\n",
    "c_URLextern = 0 \n",
    "\n",
    "for tweet in tweets_personal[\"tweet_proc\"]:\n",
    "    #print(tweet)\n",
    "    \n",
    "    # string-tweet to list\n",
    "    tweet = ast.literal_eval(tweet)\n",
    "    tweet = [n.strip() for n in tweet]\n",
    "    \n",
    "    \n",
    "    if \"USER\" in tweet:\n",
    "        c_USER += 1\n",
    "        \n",
    "    if \"USER\" == tweet[0]:\n",
    "        c_USER_START += 1\n",
    "        \n",
    "    if \"URL\" in tweet:\n",
    "        c_URL += 1\n",
    "        \n",
    "    if \"URL\" in tweet[-1]:\n",
    "        c_URL_END += 1\n",
    "    \n",
    "    if check_personal_in_tweet(tweet):\n",
    "        c_ME += 1\n",
    "        \n",
    "    if check_personal_in_tweet(tweet):\n",
    "        c_USER_ME += 1\n",
    "    elif \"USER\" in tweet[0]:\n",
    "        c_USER_ME +=1\n",
    "        \n",
    "    if check_personal_in_tweet(tweet):\n",
    "        c_ME_USER_URLnotextern += 1\n",
    "    elif \"USER\" in tweet[0]:\n",
    "        c_ME_USER_URLnotextern += 1\n",
    "    elif tweet[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "        c_ME_USER_URLnotextern += 1 \n",
    "        \n",
    "    if not tweet[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "        c_URLextern += 1\n",
    "\n",
    "print(\"count USER:\", c_USER)\n",
    "print(\"USER at start:\", c_USER_START)\n",
    "print(\"count URL:\", c_URL)\n",
    "print(\"URL at end:\", c_URL_END)\n",
    "print(\"count ME:\", c_ME)\n",
    "print(\"cout USER or ME\", c_USER_ME)\n",
    "print(\"URL extern:\", c_URLextern)\n",
    "print(\"ME USER URLnotExtern:\", c_ME_USER_URLnotextern)\n",
    "print(\"personal tweets total:\", len(tweets_personal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER start: 241  total: 269\n",
      "URL end: 229  total: 245\n",
      "personal words: 368\n",
      "personal words or USER at start 499\n",
      "URL to external source: 69\n",
      "URL to not external source: 151\n",
      "personal words or USER or Not external source: 531\n",
      "personal tweets total: 645\n"
     ]
    }
   ],
   "source": [
    "c_USER = 0\n",
    "c_USER_START = 0\n",
    "c_URL = 0\n",
    "c_URL_END = 0\n",
    "c_ME = 0\n",
    "c_USER_ME = 0\n",
    "c_ME_USER_URLnotextern = 0\n",
    "c_URLextern = 0 \n",
    "c_URLnotextern = 0 \n",
    "\n",
    "for i,user in enumerate(tweets_personal[\"user_name\"]):\n",
    "    tweet_info = manually_labelled_tweets.find({'user.screen_name' : user})[0]\n",
    "\n",
    "    tweet = tweets_personal[\"tweet_proc\"].iloc[i]\n",
    "    #print(tweet)    \n",
    "    #print(type(tweet))\n",
    "\n",
    "    # string-tweet to list\n",
    "    tweet = ast.literal_eval(tweet)\n",
    "    tweet = [n.strip() for n in tweet]\n",
    "    #print(type(tweet))\n",
    "\n",
    "    if \"USER\" in tweet:\n",
    "        c_USER += 1\n",
    "\n",
    "    if \"USER\" == tweet[0]:\n",
    "        c_USER_START += 1\n",
    "        \n",
    "    if \"URL\" in tweet:\n",
    "        c_URL += 1\n",
    "        \n",
    "    if \"URL\" in tweet[-1]:\n",
    "        c_URL_END += 1\n",
    "    \n",
    "    if check_personal_in_tweet(tweet):\n",
    "        c_ME += 1\n",
    "        \n",
    "    if check_personal_in_tweet(tweet):\n",
    "        c_USER_ME += 1\n",
    "        c_ME_USER_URLnotextern += 1\n",
    "            \n",
    "    elif \"USER\" in tweet[0]:\n",
    "        c_USER_ME +=1\n",
    "        c_ME_USER_URLnotextern += 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "\n",
    "            if tweet_info[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "                c_ME_USER_URLnotextern += 1 \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:     \n",
    "        if not tweet_info[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "            c_URLextern += 1\n",
    "            \n",
    "        if tweet_info[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "            c_URLnotextern += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"USER start:\", c_USER_START, \" total:\", c_USER)\n",
    "print(\"URL end:\", c_URL_END, \" total:\", c_URL)\n",
    "print(\"personal words:\", c_ME)\n",
    "print(\"personal words or USER at start\", c_USER_ME)\n",
    "print(\"URL to external source:\", c_URLextern)\n",
    "print(\"URL to not external source:\", c_URLnotextern)\n",
    "print(\"personal words or USER or Not external source:\", c_ME_USER_URLnotextern)\n",
    "print(\"personal tweets total:\", len(tweets_personal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER start: 42  total: 121\n",
      "URL end: 483  total: 578\n",
      "personal words: 14\n",
      "personal words or USER at start 53\n",
      "URL to external source: 434\n",
      "URL to not external source: 127\n",
      "personal words or USER or Not external source: 169\n",
      "institut tweets total: 669\n"
     ]
    }
   ],
   "source": [
    "c_USER = 0\n",
    "c_USER_START = 0\n",
    "c_URL = 0\n",
    "c_URL_END = 0\n",
    "c_ME = 0\n",
    "c_USER_ME = 0\n",
    "c_ME_USER_URLnotextern = 0\n",
    "c_URLextern = 0 \n",
    "c_URLnotextern = 0 \n",
    "\n",
    "for i,user in enumerate(tweets_institut[\"user_name\"]):\n",
    "    tweet_info = manually_labelled_tweets.find({'user.screen_name' : user})[0]\n",
    "\n",
    "    tweet = tweets_institut[\"tweet_proc\"].iloc[i]\n",
    "    #print(tweet)    \n",
    "    #print(type(tweet))\n",
    "\n",
    "    # string-tweet to list\n",
    "    tweet = ast.literal_eval(tweet)\n",
    "    tweet = [n.strip() for n in tweet]\n",
    "    #print(type(tweet))\n",
    "\n",
    "    if \"USER\" in tweet:\n",
    "        c_USER += 1\n",
    "\n",
    "    if \"USER\" == tweet[0]:\n",
    "        c_USER_START += 1\n",
    "        \n",
    "    if \"URL\" in tweet:\n",
    "        c_URL += 1\n",
    "        \n",
    "    if \"URL\" in tweet[-1]:\n",
    "        c_URL_END += 1\n",
    "    \n",
    "    if check_personal_in_tweet(tweet):\n",
    "        c_ME += 1\n",
    "        \n",
    "    if check_personal_in_tweet(tweet):\n",
    "        c_USER_ME += 1\n",
    "        c_ME_USER_URLnotextern += 1\n",
    "            \n",
    "    elif \"USER\" in tweet[0]:\n",
    "        c_USER_ME +=1\n",
    "        c_ME_USER_URLnotextern += 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "\n",
    "            if tweet_info[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "                c_ME_USER_URLnotextern += 1 \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:     \n",
    "        if not tweet_info[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "            c_URLextern += 1\n",
    "            \n",
    "        if tweet_info[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "            c_URLnotextern += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print(\"USER start:\", c_USER_START, \" total:\", c_USER)\n",
    "print(\"URL end:\", c_URL_END, \" total:\", c_URL)\n",
    "print(\"personal words:\", c_ME)\n",
    "print(\"personal words or USER at start\", c_USER_ME)\n",
    "print(\"URL to external source:\", c_URLextern)\n",
    "print(\"URL to not external source:\", c_URLnotextern)\n",
    "print(\"personal words or USER or Not external source:\", c_ME_USER_URLnotextern)\n",
    "print(\"institut tweets total:\", len(tweets_institut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_y_pred_pers(tweet_pd):\n",
    "\n",
    "    y_pred = np.zeros(len(tweets_csv.index))\n",
    "    \n",
    "    for i,user in enumerate(tweet_pd[\"user_name\"]):\n",
    "        tweet_info = manually_labelled_tweets.find({'user.screen_name' : user})[0]\n",
    "\n",
    "        tweet = tweet_pd[\"tweet_proc\"].iloc[i]\n",
    "\n",
    "        # string-tweet to list\n",
    "        tweet = ast.literal_eval(tweet)\n",
    "        tweet = [n.strip() for n in tweet]\n",
    "        \n",
    "        if check_personal_in_tweet(tweet) or \"USER\" in tweet[0]:\n",
    "            y_pred[i] = 1\n",
    "        else:\n",
    "            try:\n",
    "\n",
    "                if tweet_info[\"entities\"][\"urls\"][-1][\"expanded_url\"].startswith(\"https://twitter.com\"):\n",
    "                    y_pred[i] = 1\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "        \n",
    "\n",
    "y_pred = calc_y_pred_pers(tweets_csv)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 614, 1.0: 700})"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.784627092846271\n"
     ]
    }
   ],
   "source": [
    "def calc_confusion_matrix(y_true, y_pred):\n",
    "    \n",
    "    # counters for the 4 types of classification\n",
    "    tp = 0  # True Positives (predicted in class and are actually in class)\n",
    "    tn = 0  # True Negatives (predicted out of class and are actually out of class)\n",
    "    fp = 0  # False Positives (predicted in class but are actually out of class)\n",
    "    fn = 0  # False Negatives (predicted out of class but are actually in class)\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        \n",
    "        if y_true[i] == 0:\n",
    "            if y_pred[i] == 0:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if y_pred[i] == 0:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "                \n",
    "    confusion_matrix = np.array([[tp, fn], [fp, tn]])\n",
    "    accuracy = (tp + tn) / (tp+fn+fp+tn)\n",
    "    \n",
    "    return confusion_matrix, accuracy\n",
    "\n",
    "confusion_matrix, accuracy = calc_confusion_matrix(tweets_csv[\"personal (0=no, 1=yes)\"], y_pred)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADD Features to matrix for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1314, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_hashtags</th>\n",
       "      <th>n_urls</th>\n",
       "      <th>n_user_mentions</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1643</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3118</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  n_hashtags n_urls n_user_mentions followers_count friends_count\n",
       "0          0      1               0              29            18\n",
       "1          1      1               0               0             0\n",
       "2          2      0               0            1643          1001\n",
       "3          5      1               0             226           274\n",
       "4          0      1               0            3118           784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get meta-data matrix\n",
    "\n",
    "meta_data_pd = pd.DataFrame(columns=[\"n_hashtags\", \"n_urls\", \"n_user_mentions\", \n",
    "                                     \"followers_count\", \"friends_count\"])\n",
    "for i, user_name in enumerate(tweets_csv[\"user_name\"]):\n",
    "    for user in manually_labelled_tweets.find({'user.screen_name' : user_name}):\n",
    "        meta_data_pd.loc[i] = [len(user[\"entities\"]['hashtags']),\n",
    "                               len(user[\"entities\"]['urls']),\n",
    "                               len(user[\"entities\"]['user_mentions']),\n",
    "                               user[\"user\"]['followers_count'],\n",
    "                               user[\"user\"]['friends_count']]\n",
    "print(meta_data_pd.shape)\n",
    "meta_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     898\n",
      "1     193\n",
      "2     115\n",
      "3      50\n",
      "4      26\n",
      "5      13\n",
      "6       8\n",
      "7       5\n",
      "8       3\n",
      "9       2\n",
      "10      1\n",
      "Name: n_hashtags, dtype: int64\n",
      "1    750\n",
      "0    533\n",
      "2     30\n",
      "3      1\n",
      "Name: n_urls, dtype: int64\n",
      "0    924\n",
      "1    294\n",
      "2     68\n",
      "3     16\n",
      "4      8\n",
      "5      4\n",
      "Name: n_user_mentions, dtype: int64\n",
      "0        16\n",
      "2        12\n",
      "1         8\n",
      "4         8\n",
      "6         7\n",
      "3         7\n",
      "42        7\n",
      "10        7\n",
      "9         6\n",
      "30        6\n",
      "12        6\n",
      "16        5\n",
      "76        5\n",
      "98        5\n",
      "31        5\n",
      "29        5\n",
      "138       5\n",
      "11        5\n",
      "5         4\n",
      "190       4\n",
      "50        4\n",
      "52        4\n",
      "177       4\n",
      "130       4\n",
      "35        4\n",
      "105       4\n",
      "32        4\n",
      "66        4\n",
      "26        4\n",
      "44        4\n",
      "         ..\n",
      "11188     1\n",
      "946       1\n",
      "993       1\n",
      "21475     1\n",
      "3044      1\n",
      "997       1\n",
      "1052      1\n",
      "1051      1\n",
      "1049      1\n",
      "1045      1\n",
      "5135      1\n",
      "1035      1\n",
      "77945     1\n",
      "1030      1\n",
      "1029      1\n",
      "62468     1\n",
      "1024      1\n",
      "1020      1\n",
      "2875      1\n",
      "5112      1\n",
      "1013      1\n",
      "1012      1\n",
      "1010      1\n",
      "1007      1\n",
      "3054      1\n",
      "3053      1\n",
      "343       1\n",
      "3049      1\n",
      "999       1\n",
      "659       1\n",
      "Name: followers_count, Length: 917, dtype: int64\n",
      "0         22\n",
      "68         6\n",
      "39         6\n",
      "40         6\n",
      "74         6\n",
      "1          6\n",
      "2          6\n",
      "36         5\n",
      "404        5\n",
      "33         5\n",
      "109        5\n",
      "22         5\n",
      "335        5\n",
      "332        5\n",
      "8          5\n",
      "35         5\n",
      "312        5\n",
      "66         4\n",
      "52         4\n",
      "53         4\n",
      "63         4\n",
      "175        4\n",
      "87         4\n",
      "139        4\n",
      "195        4\n",
      "70         4\n",
      "129        4\n",
      "42         4\n",
      "107        4\n",
      "112        4\n",
      "          ..\n",
      "15160      1\n",
      "813        1\n",
      "21292      1\n",
      "811        1\n",
      "876        1\n",
      "545        1\n",
      "879        1\n",
      "4981       1\n",
      "931        1\n",
      "926        1\n",
      "925        1\n",
      "2970       1\n",
      "15254      1\n",
      "4717       1\n",
      "913        1\n",
      "136077     1\n",
      "907        1\n",
      "904        1\n",
      "4997       1\n",
      "900        1\n",
      "899        1\n",
      "898        1\n",
      "28830      1\n",
      "896        1\n",
      "894        1\n",
      "892        1\n",
      "2854       1\n",
      "4985       1\n",
      "888        1\n",
      "2605       1\n",
      "Name: friends_count, Length: 876, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(meta_data_pd[\"n_hashtags\"].value_counts())\n",
    "print(meta_data_pd[\"n_urls\"].value_counts())\n",
    "print(meta_data_pd[\"n_user_mentions\"].value_counts())\n",
    "print(meta_data_pd[\"followers_count\"].value_counts())\n",
    "print(meta_data_pd[\"friends_count\"].value_counts())\n",
    "#meta_data_pd.to_csv(\"metadata.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tweets_csv[\"personal (0=no, 1=yes)\"]\n",
    "tweets = tweets_csv[\"tweet_proc\"]\n",
    "#labels.value_counts()\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class TextAndMetaDataFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Combine tweets (label tweet) and meta-data (label metadata) \n",
    "        such as number of followers, number of friends, etc. \"\"\"\n",
    "    \n",
    "    def __init__(self, meta_data, add_metadata=True):\n",
    "        self.meta_data = meta_data \n",
    "        self.add_metadata = add_metadata\n",
    "\n",
    "    def fit(self, tweets, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, tweets):\n",
    "        \n",
    "        # add metadata if true\n",
    "        if self.add_metadata:\n",
    "            features = np.recarray(shape=(len(tweets),), \n",
    "                                   dtype=[('tweet', object), ('metadata', object)])\n",
    "\n",
    "            for i, tweet in enumerate(tweets):\n",
    "                features['tweet'][i] = tweet \n",
    "                features['metadata'][i] = np.array(self.meta_data[i].tolist())#.iloc[[i]]\n",
    "\n",
    "        # only work with tweets\n",
    "        else:\n",
    "            features = np.recarray(shape=(len(tweets),), \n",
    "                                   dtype=[('tweet', object)])\n",
    "\n",
    "            for i, tweet in enumerate(tweets):\n",
    "                features['tweet'][i] = tweet \n",
    "\n",
    "        return features\n",
    "    \n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "    \n",
    "class ArrayCaster(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Transposes meta data matrix so it fits to the tweet matrix in the feature union (pipeline)\"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, metadata):\n",
    "        metadata = metadata.tolist()\n",
    "        return csr_matrix(metadata)\n",
    "\n",
    "    \n",
    "    \n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, message=\"\"):\n",
    "        self.message = message\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(\"{}\".format(self.message))\n",
    "        print(\"shape:\", X.shape, \" type:\", type(X))\n",
    "        #print(X[0])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "#model = SVC()\n",
    "#model = RandomForestClassifier()\n",
    "pipeline  = Pipeline([\n",
    "    # combine tweets and meta-data with their labels\n",
    "    ('textMetaDataFeatureExtractor', TextAndMetaDataFeatureExtractor(meta_data_pd.values, \n",
    "                                                                     add_metadata=False)),\n",
    "    \n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            \n",
    "            # Pipeline handling the tweets\n",
    "            ('tweets', Pipeline([\n",
    "                ('tweetsSelector', ItemSelector(key='tweet')),\n",
    "                ('tfidfvect', TfidfVectorizer(lowercase=False)),\n",
    "                #('Debug1', Debug(\"tweet*****\")),\n",
    "\n",
    "                #('best', TruncatedSVC(n_components=50)),\n",
    "            ])),\n",
    "            \n",
    "            # Pipeline handling meta data\n",
    "#            ('metadata', Pipeline([\n",
    "#                ('metadataSelector', ItemSelector(key='metadata')),\n",
    "#                ('tosparse', ArrayCaster()),\n",
    "#                ('scale', StandardScaler(with_mean=False)),\n",
    "#                ('selectKbest', SelectKBest(f_classif)),\n",
    "\n",
    "                #('Debug', Debug(\"metadata *****\")),\n",
    "               # onehotencoding??#\n",
    "#            ]))\n",
    "        ]\n",
    "    )),\n",
    "    #('Debug2', Debug(\"after feature union\")),\n",
    "    # classifier\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "#print(\"tweets.shape start:\", tweets.shape)\n",
    "#pipeline.fit_transform(tweets, labels)\n",
    "#a = pipeline.predict(tweets)\n",
    "#scores = cross_val_score(pipeline, tweets, labels, cv=10, scoring='accuracy')\n",
    "#print(scores)\n",
    "#print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_START_METHOD=forkserver\n"
     ]
    }
   ],
   "source": [
    "%env JOBLIB_START_METHOD=forkserver\n",
    "#%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_START_METHOD=\"forkserver\"\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {#'union__tweets__tfidfvect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'union__tweets__tfidfvect__min_df' : [1, 5],\n",
    "              'union__tweets__tfidfvect__max_df' : [0.9, 1.0],\n",
    "              'union__tweets__tfidfvect__use_idf': (True, False),\n",
    "              'union__tweets__tfidfvect__smooth_idf': (True, False),\n",
    "              'union__tweets__tfidfvect__sublinear_tf': (True, False),\n",
    "\n",
    "              #'union__metadata__scale__with_std': (True, False),\n",
    "              #'union__metadata__selectKbest__k': [0, 2, 'all'],\n",
    "\n",
    "              \n",
    "              # param for MultinomialNB\n",
    "              'model__alpha': (10, 1, 1e-1, 1e-2),\n",
    "              \n",
    "              # param for SVC\n",
    "              #'model__kernel' : [\"linear\", \"poly\", \"rbf\"],\n",
    "              #'model__C' : [0.1, 1.0, 10],\n",
    "              #'model__tol' : [1e-5, 1e-4, 1e-3],\n",
    "              \n",
    "              # param for RandomForestClassifier\n",
    "              #'model__n_estimators' : [20, 30, 40],\n",
    "              #'model__criterion' : ['gini', 'entropy'],\n",
    "              #'model__max_features' : ['auto', 'log2', None],\n",
    "              #'model__max_depth' : [10, 20, 30]\n",
    "}\n",
    "\n",
    "# GridSearchCV gets stuck for n_jobs != 1.\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/5115\n",
    "# To fix bug set environment variable: export JOBLIB_START_METHOD=\"forkserver\"\n",
    "%env JOBLIB_START_METHOD=\"forkserver\"\n",
    "grid = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=2)\n",
    "grid = grid.fit(tweets, labels)\n",
    "\n",
    "#print(grid.cv_results_)\n",
    "print(\"\\nBest: %f using %s\" % (grid.best_score_, grid.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results with features\n",
    "# MultinomialNB Best: 0.801370 using {'union__tweets__tfidfvect__ngram_range': (1, 2), 'union__metadata__scale__with_std': True, \n",
    "#                                     'union__tweets__tfidfvect__use_idf': False, 'union__tweets__tfidfvect__min_df': 5, \n",
    "#                                     'union__tweets__tfidfvect__smooth_idf': True, 'model__alpha': 0.01, \n",
    "#                                     'union__tweets__tfidfvect__sublinear_tf': True, 'union__tweets__tfidfvect__max_df': 0.9}\n",
    "\n",
    "# Results without features\n",
    "# MultinomialNB Best: 0.846271 using {'union__tweets__tfidfvect__sublinear_tf': True, 'union__tweets__tfidfvect__ngram_range': (1, 1),\n",
    "#                                     'union__tweets__tfidfvect__min_df': 1, 'union__tweets__tfidfvect__use_idf': False, \n",
    "#                                     'model__alpha': 1, 'union__tweets__tfidfvect__smooth_idf': True, \n",
    "#                                     'union__tweets__tfidfvect__max_df': 0.9}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
